# redis-IO-Multiplexing
为什么redis的核心业务处理（处理命令）是单线程，但是面对千万条客户端请求，依然有着每秒数万 QPS 的高处理能力？这归功于redis的网络模型-IO多路复用+事件派发。
## 在开始介绍 Redis 之前，我们先来简单介绍下IO多路复用的epoll。
在传统的同步阻塞网络编程模型里（没有协程以前），性能上不来的根本原因在于进程线程都是笨重的家伙。让一个进(线)程只处理一个用户请求确确实实是有点浪费
就比如饭店的服务员和顾客。
一个进(线)程同时只能处理一个用户请求，就相当于一个服务员只能照顾一个顾客，服务员为这个顾客点完餐，才能去下一个顾客。如果同时来了 1000 个顾客，那就得 1000 个服务员，这样的人力成本很高。

如何高效的提升性能？很简单，就像现在扫码点餐一样，哪个顾客想点餐，只需要扫描桌子上的二维码，然后服务员就知道哪个顾客点了餐。

这就是多路复用。多路指的是许许多多个用户的网络连接。复用指的是对进(线)程的复用。换到餐厅的例子里，就是一群顾客只要一个服务员来处理就行了。

不过复用实现起来是需要特殊的 socket 事件管理机制的，一共有select，poll，epoll三种机制，最典型和高效的方案就是 epoll。放到餐厅的例子来，epoll 就相当于桌子上的二维码，顾客只需要扫描二维码，就可以通知到服务员。

关于这三种方式的底层实现，以及LUNIX如何处理， 这里不多赘述，可在主页的 **《》** 中查看。

在 epoll 的系列函数里， epoll_create 用于创建一个 epoll 对象，epoll_ctl 用来给 epoll 对象添加或者删除一个 socket。epoll_wait 就是查看它当前管理的这些 socket 上有没有可读可写事件发生。

当网卡接收到数据包后，Linux 内核通过中断或轮询的方式将数据拷贝到内核缓冲区，并触发网络协议栈进行处理。经过 IP 层、TCP 层等协议解析后，数据最终被送入对应 socket 的接收缓冲区（Receive Queue）。此时，内核会检测该 socket 是否已被某个 epoll 实例所监控。

如果该 socket 正在被 epoll 监听，并且事件类型（如 EPOLLIN）匹配，内核就会将对应的事件信息封装成 epoll_event 结构体，并添加到 epoll 的就绪队列（ready list）中。这样一来，用户线程在执行 epoll_wait() 时，内核只需检查就绪队列中是否已有事件即可，无需重新遍历所有 socket，从而实现了高效的 I/O 多路复用。
